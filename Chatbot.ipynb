{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOIWECGzQa9802XfhQpDpPA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utkichaps/Chatbot/blob/master/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QybjD4Dz9ER4",
        "colab_type": "text"
      },
      "source": [
        "#Data Retrieval and Pre-processing\n",
        "\n",
        "Here we upload the whatsapp chat data and pre-process it according to our own needs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORKCFP0UwsBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Uploading data\n",
        "from google.colab import files\n",
        "d = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUiSG_G--JSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Not being used in the program but can be implemented\n",
        "def remove_stop_words(str):\n",
        "  stop = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQtJwChu08Ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removes emojis\n",
        "def deEmojify(inputString):\n",
        "    return inputString.encode('ascii', 'ignore').decode('ascii')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_gH0bHM2PbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Formats the whatsapp data in proper format. Similar function for different social media pages can be added.\n",
        "from datetime import datetime\n",
        "def format_data(file):\n",
        "  data_n = open(file).read()\n",
        "  data_n = deEmojify(data_n)\n",
        "  data_n = data_n.splitlines()\n",
        "  sentences = []\n",
        "  for item in data_n:\n",
        "    no = item[0:8]\n",
        "    try:\n",
        "      datetime_object = datetime.strptime(no, '%m/%d/%y')\n",
        "    except ValueError:\n",
        "      continue \n",
        "    l = item.split(\"-\",1)\n",
        "    del(l[0])\n",
        "    s = l[0]\n",
        "    f = s.split(\":\",1)\n",
        "    f[0] = f[0].strip()\n",
        "    f[1] = f[1].strip()  \n",
        "    sentences.append(f)    \n",
        "  return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QamdN8TMpyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Add the whatsapp text files here (how many ever chats you need)\n",
        "f_sentences = []\n",
        "f_sentences.append(format_data('<File1.txt>'))\n",
        "f_sentences.append(format_data('<File2.txt>'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QnZ3uuOPhbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Splits data into the messages received and sent\n",
        "def split_data(data):  \n",
        "  sentences = []\n",
        "  labels = []\n",
        "  i = 0  \n",
        "  while i < len(data):    \n",
        "    if data[i][1] == '<Media omitted>':      \n",
        "      del(data[i])      \n",
        "      continue\n",
        "    if data[i][0] == '<Put_Your_Name_Here>':     #As seen on the whatsapp data\n",
        "      if i != 0 and data[i-1][0] == '<Put_Your_Name_Here>':\n",
        "        labels[len(labels)-1] += \" \" + data[i][1]\n",
        "      else:\n",
        "        labels.append(data[i][1])\n",
        "    else:\n",
        "      if i != 0 and data[i-1][0] != '<Put_Your_Name_Here>':\n",
        "        sentences[len(sentences)-1] += \" \" + data[i][1]\n",
        "      else:\n",
        "        sentences.append(data[i][1])\n",
        "    i+=1\n",
        "  if len(sentences) > len(labels):\n",
        "    labels.append('okay')     #Arbitrary value to keep data of the same size\n",
        "  elif len(sentences) < len(labels):\n",
        "    sentences.append('okay')\n",
        "  return sentences, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dU2igfLh8Sw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_sentences = []\n",
        "final_labels = []\n",
        "for item in f_sentences:  \n",
        "  s, l = split_data(item)\n",
        "  final_sentences.extend(s)\n",
        "  final_labels.extend(l)\n",
        "print(final_sentences)\n",
        "print(final_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyW4tSHS99Q8",
        "colab_type": "text"
      },
      "source": [
        "#Data Preperation\n",
        "\n",
        "Here we tokenize the messages into the appropriate format and prepare the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeifoEInn0Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "embedding_dim = 100\n",
        "max_length = 20   #This is the maximum length of a conversational sentence we will use.\n",
        "trunc_type='pre'\n",
        "padding_type='pre'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size=len(final_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YfTIKqWyfpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(oov_token = oov_tok)\n",
        "\n",
        "tokenizer.fit_on_texts(final_sentences)\n",
        "tokenizer.fit_on_texts(final_labels)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(len(word_index))\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(final_sentences)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "sequences_l = tokenizer.texts_to_sequences(final_labels)\n",
        "padded2 = pad_sequences(sequences_l, maxlen=1, padding='post', truncating='post')\n",
        "ys = tf.keras.utils.to_categorical(padded2, num_classes=len(word_index))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb0AUj1m196i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(ys))\n",
        "print(ys[0])\n",
        "print(len(ys[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugl-jETPbgCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = 1208\n",
        "print(final_sentences[index])\n",
        "print(final_labels[index])\n",
        "print(padded[index])\n",
        "print(padded2[index])\n",
        "print(word_index)\n",
        "print(len(word_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCBdCzJS-cst",
        "colab_type": "text"
      },
      "source": [
        "#Model 1\n",
        "\n",
        "This model predicts a one word reply based on the input sentence. We will use the predictions of this model for the input to the second model.\n",
        "\n",
        "This model trains on the messages received as the input and the first word of the messages sent as output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtocsN1dccrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "       tf.keras.layers.Embedding((len(word_index)), 64, input_length=20),\n",
        "       tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150, return_sequences = True)),\n",
        "       tf.keras.layers.LSTM(100),\n",
        "       tf.keras.layers.Dense((len(word_index)+1)/2, activation='relu'),\n",
        "       tf.keras.layers.Dense(len(word_index), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWo1sQmyd2YT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(padded, ys, epochs=50, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5ShEZ5gorlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('saved_model/current_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcLkaVvesrbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r file.zip saved_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPpvP4H8tSk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('file.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylIZroUTl7C5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This code tests the first model to see its responses\n",
        "w = \"hi\"\n",
        "print(\"Type END to end\")\n",
        "while w != 'END':\n",
        "  l = []\n",
        "  w = input()\n",
        "  l.append(w)\n",
        "  seq1 = tokenizer.texts_to_sequences(l)\n",
        "  padd = pad_sequences(seq1, maxlen=max_length, padding=padding_type, truncating=trunc_type)    \n",
        "  a = np.argmax(model.predict(padd), axis=-1) #Instead of model.predict_classes(padd) which is deprecated  \n",
        "  for word, index in tokenizer.word_index.items():\n",
        "      if index == a:\n",
        "        output_word = word\n",
        "        break\n",
        "  print(output_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD7pfSZy_EKM",
        "colab_type": "text"
      },
      "source": [
        "#Model 2\n",
        "\n",
        "For the second model, We train only on the messages received. Using the first model's prediction as input we build a sentence using LSTMs as a response to the input sentence by the user"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB2kVeloYHuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Will put an <EOS> or something at the end of the file to make sure the model can stop speaking whenever\n",
        "it predicts an <EOS> word. That will mark the end of conversation.\n",
        "'''\n",
        "import copy\n",
        "\n",
        "new_labels = copy.deepcopy(final_labels)\n",
        "for i in range(len(new_labels)):\n",
        "  new_labels[i] = new_labels[i] + \" <EOS>\"\n",
        "\n",
        "tokenizer.fit_on_texts(new_labels)\n",
        "word_index2 = tokenizer.word_index\n",
        "print(len(word_index2))\n",
        "\n",
        "new_sentences = tokenizer.texts_to_sequences(new_labels)\n",
        "input_sequences = []\n",
        "\n",
        "for sent in new_sentences:\n",
        "  for i in range(1, len(sent)):\n",
        "    n_gram_sequence = sent[:i+1]    \n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding='pre', truncating='pre')\n",
        "\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "ys2 = tf.keras.utils.to_categorical(label, num_classes=len(word_index2))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqinsQVECOEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = tf.keras.Sequential([\n",
        "       tf.keras.layers.Embedding((len(word_index2)), 64, input_length=19),   #Here the length is 19 as we use the last word as labels\n",
        "       tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150, return_sequences = True)),\n",
        "       tf.keras.layers.LSTM(100),\n",
        "       tf.keras.layers.Dense((len(word_index2)+1)/2, activation='relu'),\n",
        "       tf.keras.layers.Dense(len(word_index2), activation='softmax')\n",
        "])\n",
        "\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model2.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d50rTZQZEEGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model2.fit(predictors, ys2, epochs=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xlbQ5CA_z0t",
        "colab_type": "text"
      },
      "source": [
        "#Save Model\n",
        "\n",
        "Optional code to save the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaXB633KSv5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('./checkpoints/M1/model1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOlZSeN5TZWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.save_weights('./checkpoints/M2/model2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJYsyWQFTnHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r file.zip checkpoints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGNTukgRALkG",
        "colab_type": "text"
      },
      "source": [
        "#Final Output\n",
        "\n",
        "This is the code for the final output of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKuYMbxeT7Q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w = \"hi\"\n",
        "while w != 'END':\n",
        "  l = []\n",
        "  w = input()  \n",
        "  if w == 'END':\n",
        "    break\n",
        "  l.append(w)\n",
        "  seq1 = tokenizer.texts_to_sequences(l)\n",
        "  padd = pad_sequences(seq1, maxlen=max_length, padding=padding_type, truncating=trunc_type)    \n",
        "  a = np.argmax(model.predict(padd), axis=-1) #Instead of model.predict_classes(padd) which is deprecated  \n",
        "  for word, index in tokenizer.word_index.items():\n",
        "      if index == a:\n",
        "        output_word = word\n",
        "        break  \n",
        "  new_l = []\n",
        "  new_l.append(output_word)  \n",
        "  cnt = 1\n",
        "  while cnt != 20:        \n",
        "    seq2 = tokenizer.texts_to_sequences(new_l)    \n",
        "    pad2 = pad_sequences(seq2, maxlen=19, padding='pre', truncating='pre')\n",
        "    b = np.argmax(model2.predict(pad2), axis=-1)    \n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == b:\n",
        "        new_w = word\n",
        "        break  \n",
        "    if new_w == 'eos':\n",
        "      break\n",
        "    new_l[0] = new_l[0] + \" \" + new_w\n",
        "    cnt+=1\n",
        "  for item in new_l:\n",
        "    print(item,end=\" \")\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}